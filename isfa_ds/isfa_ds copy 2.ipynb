{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e562c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines import CoxPHFitter, WeibullAFTFitter, KaplanMeierFitter\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored\n",
    "from sksurv.util import Surv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from typing import Optional, Dict, Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf6e59",
   "metadata": {},
   "source": [
    "# 1) Comprendre les données et les objectifs\n",
    "\n",
    "**Entrées** :  \n",
    "- `clinical_train.csv` : Caractéristiques cliniques (BM_BLAST, WBC, ANC, MONOCYTES, HB, PLT, …) + ID.  \n",
    "- `molecular_train.csv` : Mutations géniques (GENE, EFFECT, VAF, …) + ID (plusieurs lignes par patient).  \n",
    "- `target_train.csv` : Labels de survie (OS_YEARS, OS_STATUS) + ID.  \n",
    "\n",
    "**Sorties** : Modèle de prédiction de risque (CoxPH, Weibull AFT) ; C-index ; stratification KM par décile ; calibration de la probabilité de survie à des points temporels (1 an, 2 ans, 3 ans).\n",
    "\n",
    "## 1. `clinical_train.csv` — Données cliniques\n",
    "Chaque ligne = 1 patient au diagnostic, contenant des informations hématologiques, centre, cytogénétique.\n",
    "\n",
    "| Colonne | Signification | Rôle |\n",
    "|---------|---------------|------|\n",
    "| **ID** | Code patient unique | Fusion des tables ; ne pas utiliser comme feature. |\n",
    "| **CENTER** | Centre médical (MSK, DFCI, …) | Vérifier le biais ; strata si nécessaire. |\n",
    "| **BM_BLAST** | % blast de moelle osseuse | ↑ → pronostic défavorable (invasion élevée). |\n",
    "| **WBC** | Globules blancs périphériques (×10⁹/L) | ↑ → charge de maladie, complications. |\n",
    "| **ANC** | Neutrophiles absolus | ↓ → infection, défavorable. |\n",
    "| **MONOCYTES** | Monocytes | Lié à AML M4/M5. |\n",
    "| **HB** | Hémoglobine (g/dL) | ↓ → anémie, défavorable. |\n",
    "| **PLT** | Plaquettes (×10⁹/L) | ↓ → hémorragie, défavorable. |\n",
    "| **CYTOGENETICS** | Chromosomes (ex. : del(20), t(3;9)) | Extraire risque ELN (anomalies 3q, 5q, 7q). |\n",
    "\n",
    "**Exemple** :\n",
    "| ID | CENTER | BM_BLAST | WBC | ANC | MONOCYTES | HB | PLT | CYTOGENETICS |\n",
    "|----|--------|----------|-----|-----|-----------|----|-----|--------------|\n",
    "| P132697 | MSK | 14.0 | 2.8 | 0.2 | 0.7 | 7.6 | 119 | 46,xy,del(20)(q12)[2]/46,xy[18] |\n",
    "| P132700 | MSK | 6.0 | 128 | 9.7 | 0.9 | 11.1 | 195 | 46,xx,t(3;9)(p13;q22)[10]/46,xx[10] |\n",
    "\n",
    "## 2. `molecular_train.csv` — Données de mutations géniques\n",
    "Chaque ligne = 1 mutation par patient (niveau variant).\n",
    "\n",
    "| Colonne | Signification | Utilisation |\n",
    "|---------|---------------|-------------|\n",
    "| **ID** | Code patient | Groupby pour synthétiser. |\n",
    "| **CHR/START/END** | Position chromosomique | Non nécessaire (sauf feature génomique). |\n",
    "| **REF/ALT** | Allèle de référence/alternative | Référence ; ne pas entraîner. |\n",
    "| **GENE** | Gène muté (TP53, NPM1, …) | Créer binaire (oui/non). |\n",
    "| **PROTEIN_CHANGE** | Changement protéique (p.R1262L) | Classer missense/truncating. |\n",
    "| **EFFECT** | Type (missense, frameshift, …) | Grouper délétère/neutre. |\n",
    "| **VAF** | % allèle muté | Quantitatif (taille clone). |\n",
    "| **DEPTH** | Profondeur de lecture | Vérifier qualité. |\n",
    "\n",
    "**Exemple** :\n",
    "| ID | GENE | EFFECT | VAF | DEPTH |\n",
    "|----|------|--------|-----|-------|\n",
    "| P100000 | CBL | non_synonymous_codon | 0.083 | 1308 |\n",
    "| P100000 | DNMT3A | frameshift_variant | 0.0898 | 942 |\n",
    "| P100000 | TET2 | non_synonymous_codon | 0.43 | 826 |\n",
    "\n",
    "### Synthèse par patient (groupby ID)\n",
    "| Caractéristique | Calcul | Signification |\n",
    "|-----------------|--------|---------------|\n",
    "| `n_genes` | Nombre de gènes mutés uniques | Complexité génétique. |\n",
    "| `vaf_mean` | Moyenne VAF | Clone moyen. |\n",
    "| `vaf_max` | VAF maximum | Clone majoritaire. |\n",
    "| `TP53_mut`, `NPM1_mut`, … | 1 si mutation présente | Stratification pronostic. |\n",
    "\n",
    "## 3. `target_train.csv` — Données de survie\n",
    "| Colonne | Signification | Rôle |\n",
    "|---------|---------------|------|\n",
    "| **ID** | Code patient | Fusion. |\n",
    "| **OS_YEARS** | Temps de survie/suivi (années) | Duration pour modèle survie. |\n",
    "| **OS_STATUS** | 1=décès ; 0=censure | Event pour modèle survie. |\n",
    "\n",
    "## 4. Signification médicale\n",
    "| Variable | Type | Signification clinique | Impact |\n",
    "|----------|------|------------------------|--------|\n",
    "| **BM_BLAST** | Clinique | Invasion moelle | ↑ → défavorable |\n",
    "| **WBC** | Clinique | Charge de maladie | ↑ → défavorable |\n",
    "| **HB** | Clinique | Anémie | ↓ → défavorable |\n",
    "| **PLT** | Clinique | Hémorragie | ↓ → défavorable |\n",
    "| **n_genes** | Moléculaire | Complexité génétique | ↑ → défavorable |\n",
    "| **vaf_max** | Moléculaire | Clone principal | ↑ → défavorable |\n",
    "| **TP53_mut** | Moléculaire | Malignité élevée | ↑ → défavorable |\n",
    "| **NPM1_mut**, **CEBPA_mut**, **IDH1/2_mut** | Moléculaire | Favorable | ↓ → favorable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa877fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = pd.read_csv(\"./data/clinical_train.csv\")\n",
    "molecular = pd.read_csv(\"./data/molecular_train.csv\")\n",
    "target = pd.read_csv(\"./data/target_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8332c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecular.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1970bb2",
   "metadata": {},
   "source": [
    "# 2) Normalisation des étiquettes et fusion des données\n",
    "\n",
    "* Fusionner `clinical` avec `target` par `ID`.\n",
    "* À partir de `molecular`, synthétiser par **patient** (par `ID`) :\n",
    "\n",
    "  * Nombre de gènes différents (`n_genes`), moyenne/max VAF (`vaf_mean`, `vaf_max`).\n",
    "  * (Optionnel) Compter les variants par EFFECT (missense/stop_gained/frameshift/splice…), et one-hot encoder les gènes AML importants (NPM1, FLT3, TP53, DNMT3A, IDH1/2, RUNX1, TET2, NRAS, KRAS, CEBPA).\n",
    "* Fusionner les caractéristiques moléculaires synthétisées dans le tableau clinique-objectif → créer **tableau de travail**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660eae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = target.copy()\n",
    "# t[\"OS_STATUS_BIN\"] = t[\"OS_STATUS\"].fillna(0)\n",
    "\n",
    "clin = clinical.merge(t[[\"ID\",\"OS_YEARS\",\"OS_STATUS\"]], on=\"ID\", how=\"left\")\n",
    "\n",
    "# Tổng hợp phân tử\n",
    "agg_f = molecular.groupby(\"ID\").agg(\n",
    "    n_genes=(\"GENE\",\"nunique\"),\n",
    "    vaf_mean=(\"VAF\",\"mean\"),\n",
    "    vaf_max=(\"VAF\",\"max\")\n",
    ").reset_index()\n",
    "agg_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gộp\n",
    "full = clin.merge(agg_f, on=\"ID\", how=\"left\")\n",
    "full = full.fillna(0)\n",
    "full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75a9c2",
   "metadata": {},
   "source": [
    "# 3) Nettoyage et prétraitement des caractéristiques\n",
    "\n",
    "1. **Valeurs manquantes et infinies**\n",
    "\n",
    "   * Remplacer `±inf` par manquant ; imputer les manquants pour les variables explicatives (suggestion : médiane) mais **ne pas imputer** pour `OS_YEARS`, `OS_STATUS_BIN`.\n",
    "\n",
    "2. **Temps non positif**\n",
    "\n",
    "   * Pour AFT, exiger `OS_YEARS > 0`. Si valeurs ≤ 0, ajouter un epsilon très petit pour assurer positivité.\n",
    "\n",
    "3. **Gestion de la skewness et des outliers**\n",
    "\n",
    "   * Variables très skewées (WBC, ANC, PLT, VAF…) devraient utiliser **log1p** ou transformation similaire.\n",
    "\n",
    "   * **Winsorize**/clip aux percentiles fins (ex. 0.5%–99.5%) pour réduire l'impact des outliers extrêmes.\n",
    "\n",
    "4. **Supprimer variables peu informatives**\n",
    "\n",
    "   * Supprimer variables avec **variance proche de 0** ou **presque une seule valeur** (bruit).\n",
    "\n",
    "5. **Réduire la multicolinéarité**\n",
    "\n",
    "   * Vérifier matrice de corrélation ; supprimer l'une des deux variables avec |ρ| trop élevé (ex. > 0.95).\n",
    "\n",
    "   * (Optionnel) Calculer **VIF**, supprimer progressivement variables avec VIF trop grand (ex. > 10).\n",
    "\n",
    "6. **Normalisation des échelles**\n",
    "\n",
    "   * Scaler (z-score) toutes les caractéristiques d'entrée pour stabiliser l'optimisation, faciliter comparaison des coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duration_col = \"OS_YEARS\"\n",
    "event_col    = \"OS_STATUS\"\n",
    "features     = [\"BM_BLAST\",\"WBC\",\"ANC\",\"MONOCYTES\",\"HB\",\"PLT\",\"n_genes\",\"vaf_mean\",\"vaf_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = full[[duration_col, event_col] + features].copy()\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remplir les NaN pour X, assurer la présence de y\n",
    "for c in features:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "df = df.dropna(subset=[duration_col, event_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb14fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Si temps ≤ 0, ajuster à epsilon pour AFT\n",
    "epsilon = 1e-6\n",
    "df.loc[df[duration_col] <= 0, duration_col] = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53343b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"WBC\",\"ANC\",\"PLT\",\"vaf_mean\",\"vaf_max\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = np.log1p(df[c])\n",
    "\n",
    "low_q, high_q = 0.005, 0.995\n",
    "for c in features:\n",
    "    lo, hi = df[c].quantile(low_q), df[c].quantile(high_q)\n",
    "    df[c] = df[c].clip(lo, hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop near-zero variance\n",
    "nzv = [c for c in features if (df[c].std() < 1e-8 or df[c].nunique() <= 2)]\n",
    "features = [c for c in features if c not in nzv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f14679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop high corr\n",
    "corr = df[features].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones_like(corr), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "features = [c for c in features if c not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "print(\"\\nCaractéristiques conservées :\", features)\n",
    "print(\"Taux d'événements :\", df[event_col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f9f28",
   "metadata": {},
   "source": [
    "# 5) Division de l'ensemble de données en garantissant la présence d'événements\n",
    "\n",
    "**Problème** : Les données de survie sont facilement déséquilibrées lors d'une division aléatoire, menant à un ensemble de test sans événements (N₁^(test) = 0).\n",
    "\n",
    "**Traitement** : Stratification par événement (δ_i) pour maintenir un taux de mortalité r = N₁/N équivalent entre train/test :\n",
    "$$\n",
    "\\frac{N_1^{(train)}}{N^{(train)}} \\approx \\frac{N_1^{(test)}}{N^{(test)}} \\approx r.\n",
    "$$\n",
    "\n",
    "Algorithme de stratification :\n",
    "1. Diviser les données : D₁ = {i : δ_i=1}, D₀ = {i : δ_i=0}.\n",
    "2. Échantillonner aléatoirement un ratio p de chaque groupe : D₁^(train) = sample(D₁, p), D₀^(train) = sample(D₀, p).\n",
    "3. Fusionner : D^(train) = D₁^(train) ∪ D₀^(train) ; D^(test) = D \\ D^(train).\n",
    "\n",
    "→ Assurer une distribution d'événements similaire.\n",
    "\n",
    "**Contraintes** : Train/test ≥ N_min événements (N_min ≈ 5–10). Si manque dans test : réduire test_size ou transférer quelques événements de train vers test.\n",
    "\n",
    "**Avec données petites/r rares** (N₁^(test/train) < N_min) :  \n",
    "- (a) Réduire test_size (ex. : 0.3 → 0.2).  \n",
    "- (b) Transférer aléatoirement des événements de train vers test jusqu'à suffisance.  \n",
    "\n",
    "→ Assurer la validité pour les modèles Cox/AFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62afccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stratified_surv_split(df, event_col=\"OS_STATUS\",\n",
    "                          test_size=0.25, min_events_test=5,\n",
    "                          max_tries=80, random_state=123):\n",
    "    y = df[event_col].astype(int).values\n",
    "    ts = test_size\n",
    "    for i in range(max_tries):\n",
    "        tr, te = train_test_split(df, test_size=ts, random_state=random_state+i, stratify=y)\n",
    "        if tr[event_col].sum() >= min_events_test and te[event_col].sum() >= min_events_test:\n",
    "            return tr, te\n",
    "        ts = max(0.10, ts - 0.05)\n",
    "    return tr, te  # fallback\n",
    "\n",
    "train_df, test_df = stratified_surv_split(df, event_col=event_col, test_size=0.25, min_events_test=5)\n",
    "print(\"events train/test:\", int(train_df[event_col].sum()), int(test_df[event_col].sum()))\n",
    "\n",
    "# nếu vẫn đen đủi, ép chuyển 1 ít event sang test:\n",
    "if test_df[event_col].sum() == 0 and train_df[event_col].sum() > 0:\n",
    "    pos_idx = train_df[train_df[event_col]==1].sample(n=min(5, int(train_df[event_col].sum())), random_state=7).index\n",
    "    move_rows = train_df.loc[pos_idx]\n",
    "    train_df = train_df.drop(index=pos_idx)\n",
    "    test_df  = pd.concat([test_df, move_rows], axis=0)\n",
    "\n",
    "print(\"events train/test (after fix):\", int(train_df[event_col].sum()), int(test_df[event_col].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55124f3a",
   "metadata": {},
   "source": [
    "\n",
    "# 6) Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74aef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols        = features  # đã chuẩn bị ở các bước trước"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabb741",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be4d57",
   "metadata": {},
   "source": [
    "## 6.1) Fit models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784d76e",
   "metadata": {},
   "source": [
    "### 6.1.1) Modèle des Hasards Proportionnels de Cox (CoxPH)\n",
    "\n",
    "**Description courte** : Ajouter un **pénaliseur ridge** faible (L2) pour stabiliser en cas de multicolinéarité/peu d'événements. Utiliser le **hasard partiel** comme mesure de score de risque.\n",
    "\n",
    "Le CoxPH est le modèle central pour l'analyse de survie, estimant l'impact des caractéristiques (X) sur le taux de hasard au fil du temps.\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Hasard pour l'individu i :\n",
    "$$\n",
    "h_i(t \\mid X_i) = h_0(t) \\exp(\\beta^\\top X_i)\n",
    "$$\n",
    "- $h_0(t)$ : hasard de base.\n",
    "- $\\beta$ : coefficients à apprendre.\n",
    "- Hypothèse : hasard proportionnel fixe entre les individus (proportional hazards).\n",
    "\n",
    "Partial Likelihood\n",
    "\n",
    "Estimer $\\beta$ par partial likelihood (ignorant $h_0(t)$) :\n",
    "$$\n",
    "L(\\beta) = \\prod_{i:\\delta_i=1} \\frac{\\exp(\\beta^\\top X_i)}{\\sum_{j \\in R(T_i)} \\exp(\\beta^\\top X_j)}\n",
    "$$\n",
    "Log-likelihood :\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i:\\delta_i=1} \\left[ \\beta^\\top X_i - \\log \\sum_{j \\in R(T_i)} \\exp(\\beta^\\top X_j) \\right]\n",
    "$$\n",
    "Maximiser $\\ell(\\beta)$ pour obtenir $\\hat{\\beta}$.\n",
    "\n",
    "Vấn đề\n",
    "\n",
    "Avec p variables ≈ N₁ événements ou multicolinéarité (ex. : WBC ~ BM_BLAST), la matrice hessienne $H = X^\\top W X$ est presque singulière → non convergence.\n",
    "\n",
    "Ridge Penalization (L2)\n",
    "\n",
    "Ajouter une pénalité :\n",
    "$$\n",
    "\\ell_{ridge}(\\beta) = \\ell(\\beta) - \\frac{\\lambda}{2} \\|\\beta\\|_2^2 \\quad (\\lambda \\in [0.01, 0.5])\n",
    "$$\n",
    "→ Stabiliser $\\hat{\\beta}$, réduire l'overfitting, lisser les poids.\n",
    "\n",
    "Partial Hazard (Risk Score)\n",
    "\n",
    "$$\n",
    "\\text{partial hazard}_i = \\exp(\\hat{\\beta}^\\top X_i), \\quad \\text{risk score}_i = \\hat{\\beta}^\\top X_i\n",
    "$$\n",
    "- Comparaison : Si risk score_i > risk score_j → i a un risque plus élevé que j.\n",
    "- HR entre i et j : $\\exp(\\hat{\\beta}^\\top (X_i - X_j))$.\n",
    "- Applications : Groupement (Kaplan–Meier), C-index, graphique de calibration.\n",
    "\n",
    "Quy trình thực hành\n",
    "\n",
    "| Étape | Objectif | Réalisation |\n",
    "|-------|----------|-------------|\n",
    "| 1. Z-score X | Éviter que les variables grandes dominent | $X_j' = \\frac{X_j - \\mu_j}{\\sigma_j}$ |\n",
    "| 2. Ridge faible | Stabiliser | `CoxPHFitter(penalizer=0.1, robust=True)` |\n",
    "| 3. Vérifier convergence | Éviter singulier | Réduire variables/augmenter λ si nécessaire |\n",
    "| 4. Prédire | Extraire risque | `cph.predict_partial_hazard(X_test)` |\n",
    "\n",
    "Exemple de résultats\n",
    "\n",
    "| Variable | $\\hat{\\beta}$ | HR = exp($\\hat{\\beta}$) | Interprétation |\n",
    "|----------|-----------------|---------------------------|----------------|\n",
    "| BM_BLAST | +0.031 | 1.03 | +1% blast → hasard ↑3% |\n",
    "| WBC | +0.008 | 1.01 | WBC ↑ → risque ↑ léger |\n",
    "| PLT | -0.004 | 0.996 | Plaquettes hautes → risque ↓ |\n",
    "| TP53_mut | +0.85 | 2.34 | Mutation présente → risque ×2.3 |\n",
    "\n",
    "Tổng kết\n",
    "\n",
    "| Composant | Expression | Signification |\n",
    "|-----------|------------|---------------|\n",
    "| Hasard | $h_i(t) = h_0(t) e^{\\beta^\\top X_i}$ | Vitesse de mortalité instantanée |\n",
    "| Log partial-LL | $\\sum [\\beta^\\top X_i - \\log \\sum e^{\\beta^\\top X_j}]$ | Estimation de $\\beta$ |\n",
    "| Pénalité ridge | $-\\frac{\\lambda}{2}\\|\\beta\\|_2^2$ | Stabiliser multicolinéarité |\n",
    "| HR | $e^{\\beta^\\top(X_i - X_j)}$ | Comparer risques |\n",
    "| Hasard partiel | $\\exp(\\beta^\\top X_i)$ | Score de risque pour classement |\n",
    "\n",
    "**Conclusion** : Le CoxPH estime l'impact relatif de X sur le risque. Le ridge faible stabilise en cas de peu d'événements/multicolinéarité. Le hasard partiel sert de score de risque pour la stratification, courbe KM, C-index/calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1443cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cox (ridge pour eviter singular)\n",
    "cph = CoxPHFitter(penalizer=0.1)\n",
    "cph.fit(train_df[[duration_col, event_col] + Xcols],\n",
    "        duration_col=duration_col, event_col=event_col, robust=True)\n",
    "risk_cox = cph.predict_partial_hazard(test_df[Xcols]).values.ravel()\n",
    "risk_cox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95b8a",
   "metadata": {},
   "source": [
    "### 6.1.2) Modèle du Temps d'Échec Accéléré de Weibull (AFT)\n",
    "\n",
    "**Description courte** : Adapté quand CoxPH converge mal ou hypothèse PH non satisfaite. Utiliser **temps de survie médian prédit** ; inverser le signe pour obtenir “score de risque” (temps court → risque élevé).\n",
    "\n",
    "Le Weibull AFT est un modèle paramétrique alternatif à Cox, modélisant directement le temps de survie (T) au lieu du hasard relatif.\n",
    "\n",
    "Objectif\n",
    "\n",
    "Log du temps de survie :\n",
    "$$\n",
    "\\log(T_i) = \\beta^\\top X_i + \\sigma \\epsilon_i\n",
    "$$\n",
    "- $X_i$ : caractéristiques ; $\\beta$ : coefficients ; $\\sigma > 0$ : échelle ; $\\epsilon_i$ : bruit (Gumbel pour Weibull).\n",
    "\n",
    "Fonction de survie & Hasard\n",
    "\n",
    "Fonction de survie :\n",
    "$$\n",
    "S(t \\mid X) = \\exp\\left[-\\left(\\frac{t}{\\lambda(X)}\\right)^{\\kappa}\\right], \\quad \\lambda(X) = \\exp(\\beta^\\top X), \\quad \\kappa = 1/\\sigma\n",
    "$$\n",
    "Hasard :\n",
    "$$\n",
    "h(t \\mid X) = \\frac{\\kappa}{\\lambda(X)} \\left(\\frac{t}{\\lambda(X)}\\right)^{\\kappa - 1}\n",
    "$$\n",
    "- $\\kappa = 1$ : hasard constant ; $\\kappa > 1$ : croissant ; $\\kappa < 1$ : décroissant.\n",
    "\n",
    "Estimation\n",
    "\n",
    "Maximum de vraisemblance sur données censurées :\n",
    "$$\n",
    "\\ell(\\beta, \\sigma) = \\sum_i \\left[ \\delta_i \\log f(T_i \\mid X_i) + (1-\\delta_i) \\log S(T_i \\mid X_i) \\right]\n",
    "$$\n",
    "Implémentation : `WeibullAFTFitter().fit(df, duration_col=\"OS_YEARS\", event_col=\"OS_STATUS_BIN\")` (lifelines).\n",
    "\n",
    "Médiane de survie\n",
    "\n",
    "$$\n",
    "t_{0.5}(X) = \\lambda(X) (\\ln 2)^{1/\\kappa} = e^{\\beta^\\top X} (\\ln 2)^{1/\\kappa}\n",
    "$$\n",
    "Prédiction : `aft.predict_median(X_test)`.\n",
    "\n",
    "Score de risque\n",
    "\n",
    "Inverser le signe pour risque élevé → survie courte :\n",
    "$$\n",
    "\\text{risk}_i = -t_{0.5}(X_i) \\propto -\\exp(\\beta^\\top X_i)\n",
    "$$\n",
    "\n",
    "Quand utiliser AFT au lieu de Cox\n",
    "\n",
    "| Situation | Raison | Avantage AFT |\n",
    "|-----------|--------|--------------|\n",
    "| Cox ne converge pas | Hessienne singulière, peu d'événements | Paramétrique stable |\n",
    "| Violation PH | HR change avec le temps | Pas besoin de PH |\n",
    "| Besoin de temps absolu | Cox seulement relatif | Prédiction T directe |\n",
    "| Données petites | Variables fortement corrélées | Exploite forme paramétrique |\n",
    "\n",
    "Procédure pratique\n",
    "\n",
    "| Étape | Objectif | Réalisation |\n",
    "|-------|----------|-------------|\n",
    "| 1. Normaliser X | Convergence facile | Z-score/transformation log |\n",
    "| 2. Choisir Weibull | Distribution appropriée | `WeibullAFTFitter()` |\n",
    "| 3. Ajuster ML | Estimer $\\beta, \\sigma$ | `aft.fit(...)` |\n",
    "| 4. Prédire médiane | Prédiction personnalisée | `aft.predict_median(X_test)` |\n",
    "| 5. Inverser signe risque | Comparer avec Cox | `risk = -predict_median` |\n",
    "| 6. Évaluer | Stratification | C-index/graphique KM |\n",
    "\n",
    "Comparaison CoxPH vs. Weibull AFT\n",
    "\n",
    "| Caractéristique | **CoxPH** | **Weibull AFT** |\n",
    "|-----------------|-----------|-----------------|\n",
    "| Objectif | Hasard relatif | Temps absolu |\n",
    "| Hasard | $h_0(t) e^{\\beta^\\top X}$ | $\\frac{\\kappa}{\\lambda} (t/\\lambda)^{\\kappa-1}$ |\n",
    "| Hypothèse PH | Oui | Non obligatoire |\n",
    "| Base | Non définie | Paramétrique Weibull |\n",
    "| Score de risque | $\\exp(\\beta^\\top X)$ | $-t_{0.5} = -e^{\\beta^\\top X} (\\ln 2)^{1/\\kappa}$ |\n",
    "| Prédiction T absolue | Non | Oui (médiane/moyenne) |\n",
    "| Stabilité données petites | Moyenne | Meilleure |\n",
    "\n",
    "Signification pratique (AML)\n",
    "\n",
    "- $\\hat{t}_{0.5,i} < 2$ ans : high risk ; >5 : low risk.\n",
    "- Utiliser –médiane comme risque pour KM, C-index, calibration.\n",
    "\n",
    "**Conclusion** : Le Weibull AFT paramétrique modélise T directement, stable quand Cox converge mal/PH violée. Estime médiane de survie, inverse en score de risque pour stratification, évaluation similaire à Cox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weibull AFT\n",
    "aft = WeibullAFTFitter()\n",
    "aft.fit(train_df[[duration_col, event_col] + Xcols],\n",
    "        duration_col=duration_col, event_col=event_col)\n",
    "# median time nhỏ -> rủi ro lớn\n",
    "risk_aft = -aft.predict_median(test_df[Xcols]).values.ravel()\n",
    "risk_aft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226224d1",
   "metadata": {},
   "source": [
    "# 3) Vérification de l'hypothèse PH (après ajustement de Cox)\n",
    "\n",
    "**Description courte** : Utiliser le test/graphique de Schoenfeld pour identifier les variables violant PH ; violation grave → envisager **stratification** (couche par variable) ou **effets variant dans le temps**.\n",
    "\n",
    "La vérification de l'hypothèse des hasards proportionnels (PH) est une étape obligatoire après l'ajustement de CoxPH, assurant un ratio de hasard fixe dans le temps.\n",
    "\n",
    "## Contexte & Signification\n",
    "CoxPH suppose :\n",
    "$$\n",
    "h(t \\mid X) = h_0(t) \\exp(\\beta^\\top X) \\implies \\frac{h(t \\mid X_i)}{h(t \\mid X_j)} = \\exp[\\beta^\\top (X_i - X_j)]\n",
    "$$\n",
    "HR indépendant de t. Violation → $\\beta$ biaisé.\n",
    "\n",
    "## Quand violation PH\n",
    "| Variable | Signe | Raison clinique |\n",
    "|----------|-------|-----------------|\n",
    "| WBC | Hasard élevé au début, décroissant | Décès précoce ; survivants postérieurs → pronostic normal |\n",
    "| TP53_mut | Impact fort au début, décroissant | Traitement inverse le risque |\n",
    "| CENTER | Stratégie change avec t | Violation par centre |\n",
    "\n",
    "## Test des Résidus de Schoenfeld\n",
    "Résidus :\n",
    "$$\n",
    "r_{ij} = x_{ij} - \\frac{\\sum_{k \\in R(T_i)} x_{kj} e^{\\beta^\\top x_k}}{\\sum_{k \\in R(T_i)} e^{\\beta^\\top x_k}}\n",
    "$$\n",
    "- Si PH correct : $r_{ij}$ aléatoire autour de 0, indépendant de t.\n",
    "- Violation : $r_{ij}$ tendance avec t.\n",
    "\n",
    "## Test de Grambsch–Therneau\n",
    "Test : $H_0: Cov(r_{ij}, \\log t_i) = 0$.\n",
    "p < 0.05 → violation. Global : $\\chi^2_{\\text{global}} = \\sum_j \\chi^2_j$.\n",
    "\n",
    "Implémentation (lifelines) :\n",
    "```python\n",
    "from lifelines.statistics import proportional_hazard_test\n",
    "results = proportional_hazard_test(cph, train_df, time_transform='rank')\n",
    "results.print_summary(decimals=3)\n",
    "```\n",
    "\n",
    "Exemple :\n",
    "| Variable | $\\chi^2$ | p | Violation ? |\n",
    "|----------|------------|---|-------------|\n",
    "| BM_BLAST | 1.22 | 0.27 | Non |\n",
    "| WBC | 5.45 | 0.019 | ✅ Oui |\n",
    "| PLT | 0.31 | 0.58 | Non |\n",
    "| TP53_mut | 4.92 | 0.027 | ✅ Oui |\n",
    "| **Global** | 8.77 | 0.031 | ⚠️ Légère |\n",
    "\n",
    "## Graphique de vérification\n",
    "```python\n",
    "cph.check_assumptions(train_df, show_plots=True)\n",
    "```\n",
    "- X : log(t) ; Y : résidus.\n",
    "- Ligne verte plate → OK ; inclinée → violation (ex. : WBC descendant).\n",
    "\n",
    "## Gestion des violations\n",
    "### 7.1 Stratification\n",
    "Variable catégorielle (ex. : CENTER) :\n",
    "$$\n",
    "h(t \\mid X, \\text{strata}) = h_{0,\\text{strata}}(t) \\exp(\\beta^\\top X)\n",
    "$$\n",
    "```python\n",
    "cph.fit(df, ..., strata=[\"CENTER\"])\n",
    "```\n",
    "\n",
    "### 7.2 Effets variant dans le temps\n",
    "Variable quantitative (ex. : WBC) :\n",
    "$$\n",
    "\\beta_j(t) = \\beta_j + \\gamma_j \\log t\n",
    "$$\n",
    "```python\n",
    "df[\"WBC_logt\"] = df[\"WBC\"] * np.log(df[\"OS_YEARS\"])\n",
    "cph.fit(df, ...)\n",
    "```\n",
    "\n",
    "### 7.3 Cox par tranches\n",
    "Diviser intervalles temporels (0–2 ans, 2–5 ans, >5 ans) ; ajuster séparément par phase.\n",
    "\n",
    "Résultats d'interprétation\n",
    "| Variable | p-value | Violation ? | Mesure |\n",
    "|----------|---------|-------------|--------|\n",
    "| BM_BLAST | 0.27 | ❌ Non | Garder |\n",
    "| WBC | 0.019 | ✅ Oui | Interaction log(t) |\n",
    "| TP53_mut | 0.027 | ✅ Oui | Stratifier/time-varying |\n",
    "| Global | 0.031 | ⚠️ Légère | Vérifier plus |\n",
    "\n",
    "**Conclusion** : Vérification PH via test/graphique de Schoenfeld assure fiabilité du modèle. Violation légère : noter ; grave : stratifier/time-varying pour corriger, préserver sens HR fixe ou ajusté."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694dd70",
   "metadata": {},
   "source": [
    "# 7) Évaluation de la discrimination : C-index (priorité à Uno’s C)\n",
    "\n",
    "**Description courte** :  \n",
    "1. **Uno’s C** (IPCW) : Mesure la discrimination en cas de censure ; nécessite une cohorte de référence pour estimer la probabilité de censure. Si train insuffisant → utiliser test comme référence.  \n",
    "2. **Fallback Harrell’s C** : Si test sans événements ou erreur IPCW (peu d'échantillons après filtrage) → utiliser Harrell’s C comme mesure de secours.  \n",
    "3. **Nettoyage des entrées** : Supprimer NaN/Inf dans temps/événements/risque ; si risque sans variance → C-index peu significatif, noter.\n",
    "\n",
    "Le C-index mesure la capacité de discrimination du classement des risques dans un modèle de survie. Priorité à Uno’s C pour ajuster la censure ; fallback à Harrell’s si nécessaire.\n",
    "\n",
    "Objectif C-index\n",
    "Mesure le ratio de paires de patients concordantes : risque élevé → décès précoce.  \n",
    "Pour i, j : comparable si $T_i < T_j$ et $\\delta_i = 1$.  \n",
    "Concordante si $r_i > r_j$.  \n",
    "$$\n",
    "C = \\frac{\\text{Nombre de paires concordantes}}{\\text{Nombre de paires valides}}\n",
    "$$\n",
    "- C=1 : parfait ; C=0.5 : aléatoire ; C<0.5 : inversé.\n",
    "\n",
    "Problème de censure\n",
    "\n",
    "La censure ($\\delta_i=0$) fait perdre des paires de comparaison → biaisé. Solution : Uno’s C (IPCW) utilise des poids inverses.\n",
    "\n",
    "Uno’s C (IPCW)\n",
    "\n",
    "Estimer $G(t) = P(C > t)$ par Kaplan–Meier sur cohorte de référence (généralement train). Si train peu d'événements → utiliser test.  \n",
    "$$\n",
    "C_{Uno} = \\frac{\\sum_{i \\neq j} I(T_i < T_j) I(\\delta_i=1) \\frac{I(r_i > r_j)}{G(T_i)^2}}{\\sum_{i \\neq j} I(T_i < T_j) I(\\delta_i=1) \\frac{1}{G(T_i)^2}}\n",
    "$$\n",
    "Poids $1/G(T_i)^2$ réduit le biais.\n",
    "\n",
    "Pratique (sksurv)\n",
    "\n",
    "```python\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "y_train = Surv.from_arrays(event=train_df[event_col].astype(bool), time=train_df[duration_col])\n",
    "y_test = Surv.from_arrays(event=test_df[event_col].astype(bool), time=test_df[duration_col])\n",
    "c_uno, _ = concordance_index_ipcw(y_train, y_test, -risk_score, test_df[duration_col])\n",
    "```\n",
    "\n",
    "Fallback : Harrell’s C\n",
    "\n",
    "Si test sans événements/erreur IPCW :  \n",
    "$$\n",
    "C_{Harrell} = \\frac{\\sum_{i<j} I(T_i < T_j, \\delta_i=1) I(r_i > r_j)}{\\sum_{i<j} I(T_i < T_j, \\delta_i=1)}\n",
    "$$\n",
    "```python\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "c_harrell, _, _ = concordance_index_censored(events, times, -risk_score)\n",
    "```\n",
    "\n",
    "Nettoyage des entrées\n",
    "\n",
    "| Vérification | Raison | Traitement |\n",
    "|--------------|--------|------------|\n",
    "| NaN/Inf dans T/δ/r | Erreur de formule | Filtrer `mask_valid = np.isfinite(...)` |\n",
    "| Risque sans variance | C insignifiant | Noter “risque constant” |\n",
    "| Tous événements=0/1 | Pas de paires valides | Logger l'erreur |\n",
    "\n",
    "```python\n",
    "mask_valid = np.isfinite(times) & np.isfinite(events) & np.isfinite(risk_score)\n",
    "if np.var(risk_score) == 0: print(\"⚠️ Pas de variance dans le risque ; C insignifiant.\")\n",
    "```\n",
    "\n",
    "Notes de rapport\n",
    "\n",
    "Noter type C, cohorte de référence, événements test.  \n",
    "Exemple : `[uno] C=0.712 (ref: train; events:40) | [harrell-fallback] C=0.693`\n",
    "\n",
    "Interprétation\n",
    "\n",
    "| C-index | Évaluation | Interprétation |\n",
    "|---------|------------|----------------|\n",
    "| <0.55 | Proche aléatoire | Pas de discrimination |\n",
    "| 0.55–0.65 | Faible–Moyenne | Signal faible |\n",
    "| 0.65–0.75 | Bonne | Discrimination haut/bas |\n",
    "| 0.75–0.85 | Très bonne | Fiable |\n",
    "| >0.85 | Trop bonne | Vérifier surapprentissage |\n",
    "\n",
    "**Conclusion** : Uno’s C (IPCW) est le plus standard pour la censure, utilise des poids pour corriger le biais. Fallback à Harrell’s si non feasible. Nettoyage des entrées assure la signification ; combiner pour évaluation cohérente sur données censurées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ac181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) C-index helper ----------\n",
    "def safe_cindex(times, events, preds, ref_times, ref_events, prefer=\"uno\"):\n",
    "    \"\"\"\n",
    "    Trả về (c_index, method_note).\n",
    "    - Lọc NaN/Inf và căn chỉnh kích thước\n",
    "    - Nếu test không có event -> Harrell\n",
    "    - Nếu IPCW lỗi vì bất kỳ lý do gì -> fallback Harrell\n",
    "    - Nếu vẫn không thể, trả về (np.nan, 'skip')\n",
    "    \"\"\"\n",
    "    # ép về 1D\n",
    "    times  = np.asarray(times).reshape(-1)\n",
    "    events = np.asarray(events).reshape(-1).astype(int)\n",
    "    preds  = np.asarray(preds).reshape(-1)\n",
    "\n",
    "    # sanity mask test\n",
    "    m_test = np.isfinite(times) & np.isfinite(events) & np.isfinite(preds)\n",
    "    times2, events2, preds2 = times[m_test], events[m_test], preds[m_test]\n",
    "\n",
    "    # không đủ mẫu để tính\n",
    "    if times2.size < 2 or preds2.size < 2:\n",
    "        return np.nan, \"skip(test<2)\"\n",
    "\n",
    "    # nếu toàn cùng 1 giá trị dự báo -> c-index vô nghĩa\n",
    "    if np.nanstd(preds2) == 0:\n",
    "        # vẫn có thể trả Harrell (sẽ ~0.5 nếu ngẫu nhiên)\n",
    "        try:\n",
    "            res = concordance_index_censored(events2.astype(bool), times2, -preds2)\n",
    "            return float(res[0]), \"harrell(constant-score)\"\n",
    "        except Exception:\n",
    "            return np.nan, \"skip(constant-score)\"\n",
    "\n",
    "    # cohort tham chiếu để ước G(t); nếu train không đủ, dùng test\n",
    "    ref_times  = np.asarray(ref_times).reshape(-1)\n",
    "    ref_events = np.asarray(ref_events).reshape(-1).astype(int)\n",
    "    m_ref = np.isfinite(ref_times) & np.isfinite(ref_events)\n",
    "    ref_times2, ref_events2 = ref_times[m_ref], ref_events[m_ref]\n",
    "\n",
    "    y_test = Surv.from_arrays(event=events2.astype(bool), time=times2.astype(float))\n",
    "    if (ref_times2.size >= 2) and (ref_events2.sum() > 0):\n",
    "        y_ref = Surv.from_arrays(event=ref_events2.astype(bool), time=ref_times2.astype(float))\n",
    "    else:\n",
    "        y_ref = y_test  # fallback: dùng test để ước censoring\n",
    "\n",
    "    # không có event ở test -> Harrell\n",
    "    if events2.sum() == 0:\n",
    "        try:\n",
    "            res = concordance_index_censored(events2.astype(bool), times2, -preds2)\n",
    "            return float(res[0]), \"harrell(no-event-test)\"\n",
    "        except Exception:\n",
    "            return np.nan, \"skip(no-event-test)\"\n",
    "\n",
    "    # Ưu tiên Uno’s C; nếu lỗi thì fallback Harrell\n",
    "    if prefer == \"uno\":\n",
    "        try:\n",
    "            c, _ = concordance_index_ipcw(y_ref, y_test, -preds2, times2)\n",
    "            return float(c), \"uno\"\n",
    "        except Exception as e:\n",
    "            # fallback: Harrell\n",
    "            try:\n",
    "                res = concordance_index_censored(events2.astype(bool), times2, -preds2)\n",
    "                return float(res[0]), f\"harrell(fallback:{type(e).__name__})\"\n",
    "            except Exception:\n",
    "                return np.nan, \"skip(fallback-failed)\"\n",
    "    else:\n",
    "        # trực tiếp Harrell\n",
    "        try:\n",
    "            res = concordance_index_censored(events2.astype(bool), times2, -preds2)\n",
    "            return float(res[0]), \"harrell\"\n",
    "        except Exception:\n",
    "            return np.nan, \"skip(harrell-failed)\"\n",
    "\n",
    "# ---------- 3) Evaluate ----------\n",
    "times, events = test_df[duration_col].values, test_df[event_col].values\n",
    "ref_times, ref_events = train_df[duration_col].values, train_df[event_col].values\n",
    "\n",
    "print(\n",
    "    \"Sanity before eval |\",\n",
    "    \"len(times)=\", len(times),\n",
    "    \"len(events)=\", len(events),\n",
    "    \"len(risk_cox)=\", len(risk_cox),\n",
    "    \"NaN risk_cox=\", np.isnan(risk_cox).sum(),\n",
    "    \"Inf risk_cox=\", np.isinf(risk_cox).sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415e524",
   "metadata": {},
   "source": [
    "une étape de vérification rapide (« sanity check ») avant d'évaluer l'indice C dans un modèle de survie (survival).  \n",
    "Elle vous aide à confirmer que les données d'entrée pour la partie évaluation du modèle sont valides, sans manquants ni erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324663b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_cox, m1 = safe_cindex(times, events, risk_cox, ref_times, ref_events, prefer=\"uno\")\n",
    "c_aft, m2 = safe_cindex(times, events, risk_aft, ref_times, ref_events, prefer=\"uno\")\n",
    "print(f\"[{m1}] C-index Cox = {c_cox:.3f} | [{m2}] C-index AFT = {c_aft:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198dab6",
   "metadata": {},
   "source": [
    "# 8) Stratification des risques et visualisation\n",
    "\n",
    "**Description courte** :  \n",
    "* **Kaplan–Meier par décile de risque** : Diviser le test en 10 groupes par score de risque (Cox) ; tracer les courbes de survie pour chaque groupe. Séparation claire (groupe élevé descend rapidement) → modèle discriminatif bon.  \n",
    "* (Optionnel) **KM par mutation** : Comparer avec/sans mutation (TP53, NPM1…) ; avec test log-rank.\n",
    "\n",
    "La stratification des risques par Kaplan–Meier (KM) selon les déciles de score de risque est une façon de visualiser la capacité discriminante du modèle Cox/AFT.\n",
    "\n",
    "## Objectif\n",
    "Visualiser la discrimination des risques : risque élevé → courbe de survie descend rapidement ; risque faible → courbe de survie élevée longtemps. Écart clair → modèle bon pour la clinique.\n",
    "\n",
    "## Calcul du score de risque\n",
    "- **Cox** : $r_i = \\hat{\\beta}^\\top X_i$ (ou `predict_partial_hazard`).  \n",
    "- **AFT** : $r_i = -\\hat{T}_{\\text{median},i}$ (temps court → risque élevé).\n",
    "\n",
    "## Stratification par décile\n",
    "Diviser le test en 10 groupes selon $r_i$ :  \n",
    "$$\n",
    "G_k = \\{ i : q_{k-1} < r_i \\le q_k \\}, \\quad k=1\\dots10\n",
    "$$  \n",
    "(q_k : quantile k/10). Groupe 1 : faible ; 10 : élevé.  \n",
    "```python\n",
    "test_df[\"risk_group\"] = pd.qcut(risk_cox, q=10, labels=False) + 1\n",
    "```\n",
    "\n",
    "## Estimateur Kaplan–Meier\n",
    "Fonction de survie pour le groupe k :  \n",
    "$$\n",
    "\\hat{S}_k(t) = \\prod_{t_i \\le t} \\left(1 - \\frac{d_i^{(k)}}{n_i^{(k)}}\\right)\n",
    "$$  \n",
    "- $d_i^{(k)}$ : décès à t_i ; $n_i^{(k)}$ : survivants.  \n",
    "Hasard élevé → $\\hat{S}_k(t)$ descend rapidement.\n",
    "\n",
    "## Pratique (lifelines)\n",
    "```python\n",
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "plt.figure(figsize=(7,5))\n",
    "for g in sorted(test_df[\"risk_group\"].unique()):\n",
    "    mask = test_df[\"risk_group\"] == g\n",
    "    kmf.fit(test_df.loc[mask, \"OS_YEARS\"], \n",
    "            event_observed=test_df.loc[mask, \"OS_STATUS_BIN\"], \n",
    "            label=f\"Decile {g}\")\n",
    "    kmf.plot_survival_function(ci_show=False)\n",
    "plt.title(\"KM Curves by Risk Decile (Cox)\")\n",
    "plt.xlabel(\"Time (years)\"); plt.ylabel(\"Survival probability\")\n",
    "plt.legend(title=\"Risk group (1=Low → 10=High)\"); plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Interprétation\n",
    "| Phénomène | Interprétation |\n",
    "|-----------|----------------|\n",
    "| Groupe 10 descend rapidement | Prédiction correcte high risk |\n",
    "| Groupe 1 stable | Pronostic bon |\n",
    "| Courbes séparées, non croisées | Stratification bonne |\n",
    "| Courbes superposées | Pas de discrimination |\n",
    "| Fluctuations fortes | Échantillon petit → variabilité |\n",
    "\n",
    "## Test log-rank\n",
    "Confirmer les différences : $H_0: S_1(t) = \\dots = S_{10}(t)$.  \n",
    "```python\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "results = multivariate_logrank_test(test_df[\"OS_YEARS\"], test_df[\"risk_group\"], \n",
    "                                    event_observed=test_df[\"OS_STATUS_BIN\"])\n",
    "results.print_summary()  # p < 0.05 → différence significative\n",
    "```\n",
    "\n",
    "## Variantes\n",
    "- Tertile/quartile (3–4 groupes) si données peu nombreuses.  \n",
    "- KM par mutation (optionnel) : Comparer avec/sans TP53/NPM1 ; test log-rank.  \n",
    "  ```python\n",
    "  kmf.fit(df[mut==\"Yes\"], \"OS_YEARS\", event_observed=..., label=\"TP53 mut\")\n",
    "  kmf.plot_survival_function()\n",
    "  # Log-rank : logrank_test entre 2 groupes\n",
    "  ```\n",
    "\n",
    "## Exemple de résultats\n",
    "| Groupe | Médiane de survie (années) | p-value |\n",
    "|--------|----------------------------|---------|\n",
    "| Décile 1 (faible) | 5.8 | |\n",
    "| Décile 10 (élevé) | 0.9 | <1e-6 |\n",
    "\n",
    "Graphique : Groupe 10 (rouge) descend <1 an ; groupe 1 (bleu) >5 ans → séparation forte.\n",
    "\n",
    "**Conclusion** : KM par décile de score de risque illustre la discrimination des risques ; courbes séparées + log-rank bas → modèle fort, interprétable cliniquement. Combiner avec C-index pour évaluation complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68742625",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plot = test_df.copy()\n",
    "test_plot[\"risk_cox\"] = risk_cox\n",
    "# trong trường hợp nhiều giá trị trùng, dùng duplicates=\"drop\"\n",
    "test_plot[\"decile\"] = pd.qcut(test_plot[\"risk_cox\"], q=10, labels=False, duplicates=\"drop\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "km = KaplanMeierFitter()\n",
    "for d in sorted(test_plot[\"decile\"].unique()):\n",
    "    grp = test_plot[test_plot[\"decile\"] == d]\n",
    "    if len(grp) < 5:  # nhóm quá ít thì bỏ qua\n",
    "        continue\n",
    "    km.fit(grp[duration_col].values, grp[event_col].values, label=f\"Decile {int(d)+1}\")\n",
    "    km.plot_survival_function(ci_show=False)\n",
    "\n",
    "plt.title(\"Kaplan–Meier by risk decile (Cox)\")\n",
    "plt.xlabel(\"Years\"); plt.ylabel(\"Survival probability\")\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f913a",
   "metadata": {},
   "source": [
    "# 9) Calibration des probabilités à des points temporels (calibration)\n",
    "\n",
    "**Description courte** :  \n",
    "* Objectif : Estimer $\\hat{p}(t_0 \\mid X) = P(T > t_0 \\mid X)$ à des points temporels cliniques (1 an, 2 ans, 3 ans…).  \n",
    "* **Construire variable binaire** : y=1 si survie au-delà de t₀ (ou censure après t₀) ; exclure les censures avant t₀.  \n",
    "* **Calibration** : Platt (sigmoïde logistique) pour biais en S, simple ; Isotonique (non paramétrique) flexible mais sujet à surapprentissage si données peu nombreuses.  \n",
    "* **Évaluation** : Tracer **diagramme de fiabilité** (ligne proche de la diagonale 45° → bon).  \n",
    "* Recommandation : Séparer un « ensemble de calibration » ou utiliser validation croisée pour éviter surapprentissage.\n",
    "\n",
    "La calibration convertit le score de risque en probabilité de survie réelle, assurant predicted ≈ observed pour les prévisions cliniques.\n",
    "\n",
    "## Objectif\n",
    "Prédire la probabilité de survie au-delà de t₀ :  \n",
    "$$\n",
    "\\hat{p}(t_0 \\mid X) = \\widehat{P}(T > t_0 \\mid X)\n",
    "$$  \n",
    "Aligner le score de risque sur la fréquence de survie observée.\n",
    "\n",
    "## Préparation des données\n",
    "À t₀ (ex. : 1 an) :  \n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases} \n",
    "1 & T_i > t_0 \\ (survie/censure après) \\\\ \n",
    "0 & T_i \\le t_0, \\ \\delta_i=1 \\ (décès avant) \\\\ \n",
    "\\text{exclure} & T_i \\le t_0, \\ \\delta_i=0 \\ (censure avant)\n",
    "\\end{cases}\n",
    "$$  \n",
    "→ $\\mathcal{D}_{cal} = \\{(r_i, y_i)\\}$, r_i = risque de Cox/AFT.\n",
    "\n",
    "## Méthodes de calibration\n",
    "Trouver $f: r_i \\mapsto \\hat{p}_i = P(T > t_0 \\mid X_i)$.\n",
    "\n",
    "### Platt (sigmoïde logistique)\n",
    "Pour biais en S (trop optimiste/pessimiste aux extrémités) :  \n",
    "$$\n",
    "P(T > t_0 \\mid X) = \\frac{1}{1 + \\exp(-(a + b r))}\n",
    "$$  \n",
    "Ajuster logistique sur $\\mathcal{D}_{cal}$.  \n",
    "Avantage : Simple, peu de surapprentissage. Inconvénient : Peu flexible pour non linéarités.\n",
    "\n",
    "### Régression isotonique\n",
    "Non paramétrique, préservant la monotonicité :  \n",
    "$$\n",
    "f^* = \\arg\\min_f \\sum_i (f(r_i) - y_i)^2 \\ \\text{s.t.} \\ f(r_i) \\le f(r_j) \\ \\text{si} \\ r_i < r_j\n",
    "$$  \n",
    "Avantage : Flexible. Inconvénient : Surapprentissage si peu de données (<100 événements).\n",
    "\n",
    "## Évaluation : Diagramme de fiabilité\n",
    "Diviser en 10 groupes selon $\\hat{p}_i$ :  \n",
    "| Notation | Signification |  \n",
    "|----------|---------------|  \n",
    "| $\\bar{p}_j = mean(\\hat{p}_i)$ | Prédiction moyenne groupe j |  \n",
    "| $\\bar{y}_j = mean(y_i)$ | Observation réelle groupe j |  \n",
    "\n",
    "Tracer $\\bar{y}_j$ vs. $\\bar{p}_j$ :  \n",
    "- Diagonale 45° : Parfait.  \n",
    "- Sous diagonale : Trop optimiste (sur-estimation survie).  \n",
    "- Au-dessus diagonale : Trop pessimiste (sous-estimation).\n",
    "\n",
    "## Pratique (sklearn/lifelines)\n",
    "```python\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Construire y_bin à t0=1.0\n",
    "mask_keep = (times > t0) | ((times <= t0) & (events == 1))\n",
    "y_bin = (times > t0).astype(int)[mask_keep]\n",
    "risk = risk_cox[mask_keep]\n",
    "\n",
    "# 2. Calibration\n",
    "platt = LogisticRegression().fit(risk.reshape(-1,1), y_bin)\n",
    "p_platt = platt.predict_proba(risk.reshape(-1,1))[:,1]\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds='clip').fit(risk, y_bin)\n",
    "p_iso = iso.predict(risk)\n",
    "\n",
    "# 3. Graphique de fiabilité\n",
    "plt.figure(figsize=(5,5)); plt.plot([0,1],[0,1],'--',color='gray')\n",
    "for name, p_pred in [(\"Platt\", p_platt), (\"Isotonic\", p_iso)]:\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_bin, p_pred, n_bins=10)\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, 'o-', label=name)\n",
    "plt.xlabel(\"Predicted P(T > t₀)\"); plt.ylabel(\"Observed frequency\")\n",
    "plt.legend(); plt.title(\"Calibration at 1 year\"); plt.show()\n",
    "```\n",
    "\n",
    "## Évaluation quantitative (optionnelle)\n",
    "Score de Brier à t₀ :  \n",
    "$$\n",
    "\\text{Brier}(t_0) = \\frac{1}{N} \\sum_i (I(T_i > t_0) - \\hat{p}_i)^2\n",
    "$$  \n",
    "Faible → bon. IBS : Moyenne de Brier sur le temps.\n",
    "\n",
    "## Procédure pratique\n",
    "| Étape | Objectif | Réalisation |\n",
    "|-------|----------|-------------|\n",
    "| 1. Choisir t₀ (1 an,2 ans,3 ans) | Adapté clinique | Basé sur étude |\n",
    "| 2. Créer y binaire | Problème binaire | Exclure censurés avant t₀ |\n",
    "| 3. Calibrer | Éliminer biais | Platt/Isotonique sur $\\mathcal{D}_{cal}$ |\n",
    "| 4. Évaluer | Vérifier alignement | Fiabilité + Brier |\n",
    "| 5. Éviter surapprentissage | Réfléchir hors échantillon | Ensemble calibration séparé/CV |\n",
    "\n",
    "## Pratique & recommandations\n",
    "| Situation | Méthode | Raison |\n",
    "|-----------|---------|--------|\n",
    "| Données ≥500 | Isotonique | Flexible non linéaire |\n",
    "| Données petites/peu d'événements | Platt | Stable, peu de surapprentissage |\n",
    "| Déploiement réel | Platt/spline sur hold-out | Facile à stocker/appliquer |\n",
    "| CV | `calibration_curve` par fold | Éviter chevauchement entraînement |\n",
    "\n",
    "**Conclusion** : Calibration à t₀ convertit risque en probabilité de survie réelle (y=1 si au-delà du point). Platt/Isotonique élimine biais ; diagramme de fiabilité vérifie (proche 45° → calibré). Utiliser ensemble séparé/CV évite surapprentissage → modèle de prédiction personnalisée fiable pour clinique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8628a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_at_t0(time, event, t0):\n",
    "    # y=1 nếu T>t0 (sống qua mốc) hoặc kiểm duyệt sau t0\n",
    "    y = ((time > t0) | ((time <= t0) & (event==0))).astype(int)\n",
    "    # loại các mẫu kiểm duyệt trước t0 (không biết kết cục tại t0)\n",
    "    mask = ~((time < t0) & (event==0))\n",
    "    return y[mask], mask\n",
    "\n",
    "def reliability(y_true, y_pred, bins=np.linspace(0,1,10)):\n",
    "    d = pd.DataFrame({\"y\":y_true, \"p\":y_pred})\n",
    "    d[\"bin\"] = pd.cut(d[\"p\"], bins)\n",
    "    return d.groupby(\"bin\")[[\"y\",\"p\"]].mean().dropna()\n",
    "\n",
    "for t0 in [1.0, 2.0, 3.0]:\n",
    "    y_calib, mask = make_binary_at_t0(times, events, t0)\n",
    "    sc = risk_cox[mask]\n",
    "    # bỏ qua nếu chỉ có 1 lớp (toàn sống/toàn sự kiện)\n",
    "    if len(np.unique(y_calib)) < 2 or len(y_calib) < 30:\n",
    "        print(f\"[Calibration t0={t0}] Bỏ qua (ít điểm hoặc chỉ một lớp).\")\n",
    "        continue\n",
    "\n",
    "    # Platt\n",
    "    pl = LogisticRegression(max_iter=1000).fit(sc.reshape(-1,1), y_calib)\n",
    "    p_platt = pl.predict_proba(sc.reshape(-1,1))[:,1]\n",
    "\n",
    "    # Isotonic\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\").fit(sc, y_calib)\n",
    "    p_iso = iso.transform(sc)\n",
    "\n",
    "    r_pl  = reliability(y_calib, p_platt)\n",
    "    r_iso = reliability(y_calib, p_iso)\n",
    "\n",
    "    # vẽ\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.plot(r_pl[\"p\"], r_pl[\"y\"], 'o-', label=\"Platt\")\n",
    "    plt.plot(r_iso[\"p\"], r_iso[\"y\"], 's-', label=\"Isotonic\")\n",
    "    plt.xlabel(f\"Predicted P(T>{t0}y)\")\n",
    "    plt.ylabel(\"Observed frequency\")\n",
    "    plt.title(f\"Calibration at {t0} years (Cox risk)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # lưu calibration points\n",
    "    # r_pl.assign(t0=t0, method=\"platt\").to_csv(f\"./data/calib_points_platt_t{int(t0)}.csv\")\n",
    "    # r_iso.assign(t0=t0, method=\"isotonic\").to_csv(f\"./data/calib_points_isotonic_t{int(t0)}.csv\")\n",
    "\n",
    "print(\"Đã lưu calibration points (nếu có): /mnt/data/calib_points_*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff37f9",
   "metadata": {},
   "source": [
    "# 10) Rapport et export des résultats\n",
    "\n",
    "* **Coefficients et signification Cox** : tableau de coefficients, HR = exp(coef), IC, p-value ; sauvegarder le tableau résumé.\n",
    "* **Paramètres AFT** : estimations et signification.\n",
    "* **C-index** (Uno/Harrell) pour Cox et AFT ; préciser la méthode (Uno/Harrell) et la raison du fallback.\n",
    "* **Graphiques** :\n",
    "\n",
    "  * KM par décile (Cox), KM par grandes mutations.\n",
    "  * Calibration à t₀ (1 an, 2 ans, 3 ans).\n",
    "* **Prédictions test** :\n",
    "\n",
    "  * Sauvegarder le score de risque sur test, et si nécessaire, la probabilité de survie à t₀.\n",
    "  * (Avec Cox) utiliser la survie de base pour déduire ( \\hat S(t|x) = \\hat S_0(t)^{\\exp(\\beta^\\top x)} ) et extraire spécifiquement à t₀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 4) PREDICT SURVIVAL AT t0 (Cox)\n",
    "# =====================\n",
    "# Lifelines allows computing S_hat(t|x) from baseline + partial hazard.\n",
    "# For brevity, we directly compute the probabilities at t0 for the test set.\n",
    "\n",
    "t0_list = [1.0, 2.0, 3.0]\n",
    "\n",
    "# baseline survival according to Cox\n",
    "baseline_surv = cph.baseline_survival_  # index is time (same unit as OS_YEARS)\n",
    "# Utility function: interpolate S0(t) then exponentiate according to exp(beta^T x)\n",
    "def predict_survival_prob_at_t0(cox_model, X, t0):\n",
    "    # get S0(t0) by nearest interpolation (forward fill)\n",
    "    S0 = float(baseline_surv.reindex(baseline_surv.index.union([t0])).sort_index().ffill().loc[t0].values)\n",
    "    # partial hazard = exp(beta^T x) → S(t|x) = S0(t) ** exp(beta^T x)\n",
    "    ph = cox_model.predict_partial_hazard(X).values.ravel()\n",
    "    return (S0 ** ph)\n",
    "\n",
    "pred_out = test_df[[\"OS_YEARS\",\"OS_STATUS\"]].copy()\n",
    "for t0 in t0_list:\n",
    "    pred_out[f\"P_surv_gt_{int(t0)}y_cox\"] = predict_survival_prob_at_t0(cph, test_df[Xcols], t0)\n",
    "\n",
    "pred_out[\"risk_cox\"] = risk_cox\n",
    "pred_out\n",
    "# pred_out.to_csv(\"./data/test_predictions_cox.csv\", index=False)\n",
    "# print(\"Saved: /mnt/data/test_predictions_cox.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c5702",
   "metadata": {},
   "source": [
    "# 11) Diagnostic et résolution des problèmes courants\n",
    "\n",
    "* **Matrice singulière / non convergence (Cox)** :\n",
    "\n",
    "  * Déjà log/clip/contrôle des outliers, suppression de variance proche de zéro, réduction de colinéarité (corrélation élevée, VIF), ajout de **pénaliseur ridge**.\n",
    "  * Si toujours difficile : essayer **stratification** selon la variable violant PH, ou passer à **AFT**.\n",
    "* **Test sans événements** :\n",
    "\n",
    "  * Ajuster la méthode de division (stratification + contrainte sur le nombre d'événements), ou ajouter une procédure « transfert de quelques cas d'événements » de train vers test.\n",
    "  * Si le dataset n'a vraiment pas d'événements (tous vivants/censurés) : **impossible** d'évaluer la discrimination par C-index ; rediriger vers description/seulement censure.\n",
    "* **Erreur IPCW due à tableau vide** :\n",
    "\n",
    "  * Toujours filtrer NaN/Inf avant ; si après filtrage trop peu d'observations (<2) alors noter « skip ».\n",
    "* **Scores prédits sans variance** :\n",
    "\n",
    "  * Revoir le pipeline de prétraitement ; vérifier s'il y a une étape rendant tous les scores identiques (ex. : scale erroné, suppression de colonnes, logique de risque constante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5) EXPORT EXTRAS\n",
    "# =====================\n",
    "# Importance proxy: |coef| (Cox)\n",
    "imp = cph.params_.abs().sort_values(ascending=False).rename(\"abs_coef\")\n",
    "# imp.to_csv(\"./data/cox_importance_abscoef.csv\")\n",
    "# print(\"Đã lưu: /mnt/data/cox_importance_abscoef.csv\")\n",
    "\n",
    "# Bảng tóm tắt kết quả chính\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"C-index (Cox)\", \"C-index (AFT)\"],\n",
    "    \"value\":  [c_cox, c_aft],\n",
    "    \"estimator\": [m1, m2]\n",
    "})\n",
    "# summary.to_csv(\"./data/summary_metrics.csv\", index=False)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131521fc",
   "metadata": {},
   "source": [
    "# 12) Validation et tests\n",
    "\n",
    "* **Validation croisée stratifiée par événement** (k-fold) et **CV groupée par CENTER** (si centres/hôpitaux présents) pour vérifier la généralisabilité.\n",
    "* **Analyse de sensibilité** : changements légers sur winsorize/pénaliseur/transformation log, vérifier si C-index et calibration stables.\n",
    "* **Explication du modèle** :\n",
    "\n",
    "  * Cox : basée sur HR, test PH.\n",
    "  * AFT : coefficients interprétés selon log-temps.\n",
    "  * (Optionnel) SHAP/Permutation pour modèles non linéaires (si utilisation de GBM/NN plus tard).\n",
    "\n",
    "\n",
    "# 13) Gouvernance du modèle et reproductibilité\n",
    "\n",
    "* **Enregistrer les versions** : données, liste des caractéristiques finales, paramètres du modèle, pénaliseur, t₀ calibration.\n",
    "* **Sauvegarder le pipeline** : diagramme ETL → Caractéristiques → Division → Entraînement → Évaluation → Rapport.\n",
    "* **Contrôle qualité** : checklist avant entraînement (taux d'événements, nombre de NA, corr/VIF, échelles).\n",
    "* **Reproductibilité des résultats** : sauvegarder tous les artefacts (tableau de coefficients, C-index, graphiques KM, points de calibration, fichier de prédictions test).\n",
    "\n",
    "\n",
    "## Critères « travail terminé »\n",
    "\n",
    "* Nombre total d'**événements > 0** et **événements présents dans test**.\n",
    "* C-index (Uno ou fallback Harrell) calculé avec succès pour **Cox** et **AFT**.\n",
    "* Graphique **KM par décile** montrant une stratification des risques raisonnable.\n",
    "* **Calibration** à t₀ exécutée et ligne de fiabilité proche de 45°.\n",
    "* Rapport complet exporté : coefficients Cox (HR, IC, p), paramètres AFT, tableau C-index, fichier de prédictions test, figures KM, figures de calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19205aef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VNFin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
